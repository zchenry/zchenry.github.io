---
layout: post
title: The Lottery Ticket Hypothesis
visible: True
---

I [implemented](https://github.com/zchenry/lottery-ticket-hypothesis/) the first part of the paper [The Lottery Ticket Hypothesis: Training Pruned Neural Networks](https://arxiv.org/abs/1803.03635) using [Chainer](https://github.com/chainer/chainer).

### About the Paper

The paper is about pruning a fat neural network. The outstanding part of this paper is that the authors use the **initial weight** of the pruned network, instead of the **trained weight**.

In short, say there's a neural network consists of $10$ neurons and we want to trim it to only $2$ neurons. We first train the $10$ neurons network until convergence. Then remember the place of the 2 largest weights, say the $4$th and the $7$th. Then, we use the **initial value** of the $4$th and the $7$th neuron to compose a small network of only $2$ neurons, and use it.

### About the Implementation
The workflow is simple. We just need to remember the initial weights and  iteratively prune them. Running the codes using MNIST generate the following figure.

![result](/assets/img/2018-05-18-lottery.png)

The number means how many links are **left** in the network. _Prun_ means normal pruning, _shuffle_ means shuffle the places of the left weights after pruning, and _normal_ means normally initialize the weights of the pruned architecture.

Clearly, we can see as more weights get pruned, the strategy outperforms vanilla initialization. It is very interesting if this phenomenon can be explained in terms of some theories.
